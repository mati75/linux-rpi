commit 657fbf79a88019807202c849561a25773711e84b
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Feb 12 15:35:55 2019 -0800

    Revert "mm: slowly shrink slabs with a relatively small number of objects"
    
    commit a9a238e83fbb0df31c3b9b67003f8f9d1d1b6c96 upstream.
    
    This reverts commit 172b06c32b9497 ("mm: slowly shrink slabs with a
    relatively small number of objects").
    
    This change changes the agressiveness of shrinker reclaim, causing small
    cache and low priority reclaim to greatly increase scanning pressure on
    small caches.  As a result, light memory pressure has a disproportionate
    affect on small caches, and causes large caches to be reclaimed much
    faster than previously.
    
    As a result, it greatly perturbs the delicate balance of the VFS caches
    (dentry/inode vs file page cache) such that the inode/dentry caches are
    reclaimed much, much faster than the page cache and this drives us into
    several other caching imbalance related problems.
    
    As such, this is a bad change and needs to be reverted.
    
    [ Needs some massaging to retain the later seekless shrinker
      modifications.]
    
    Link: http://lkml.kernel.org/r/20190130041707.27750-3-david@fromorbit.com
    Fixes: 172b06c32b9497 ("mm: slowly shrink slabs with a relatively small number of objects")
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Cc: Wolfgang Walter <linux@stwm.de>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Spock <dairinin@gmail.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/mm/vmscan.c b/mm/vmscan.c
index 961401c46334..3830066018c1 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -477,16 +477,6 @@ static unsigned long do_shrink_slab(struct shrink_control *shrinkctl,
 	delta *= 4;
 	do_div(delta, shrinker->seeks);
 
-	/*
-	 * Make sure we apply some minimal pressure on default priority
-	 * even on small cgroups. Stale objects are not only consuming memory
-	 * by themselves, but can also hold a reference to a dying cgroup,
-	 * preventing it from being reclaimed. A dying cgroup with all
-	 * corresponding structures like per-cpu stats and kmem caches
-	 * can be really big, so it may lead to a significant waste of memory.
-	 */
-	delta = max_t(unsigned long long, delta, min(freeable, batch_size));
-
 	total_scan += delta;
 	if (total_scan < 0) {
 		pr_err("shrink_slab: %pF negative objects to delete nr=%ld\n",
diff -uN a/1.txt b/1.txt
--- a/dummy/rpi_1423_657fbf79a88019807202c849561a25773711e84b.txt	1970-01-01 00:00:00.000000000 +0000
+++ b/dummy/rpi_1423_657fbf79a88019807202c849561a25773711e84b.txt	2013-12-23 04:07:40.000000000 +0000
@@ -0,0 +1 @@
+dummy file to ensure patch has content.
